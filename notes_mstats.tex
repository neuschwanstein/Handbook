\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{outlines}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{siunitx}
\usepackage{bm}
\usepackage{listings}

\geometry{letterpaper}

\AtBeginDocument{
\def\labelitemi{\textbullet}%
\def\labelitemii{$-$}%
\def\labelitemiii{$\star$}%
\def\labelitemiv{\maltese}
}

\newcommand{\dd}{\partial}
\newcommand{\Bfix}{B_\text{fix}}
\newcommand{\Bfl}{B_\text{fl}}
\newcommand{\Vswap}{V_\text{swap}}
\newcommand{\real}{\mathscr{R}}
\newcommand{\grad}{\nabla}
\newcommand{\normal}{\mathscr{N}}
\newcommand{\logNormal}{\mathscr{LN}}
\newcommand{\filtr}{\mathscr{F}}

\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\GammaDist}{Gamma}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Garch}{GARCH}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\chol}{chol}

\newcommand{\reff}[1]{\mbox{}~\hfill \footnotesize{[#1]}\normalsize}

\title{The Handbook of\\Mathematics, Statistics, and Various Other Topics}
\author{Thierry Bazier Matte}
\date{2015}

\begin{document}
\maketitle

\part{Applied Mathematics, Optimization and Risk Theory}

\section{Convexity and Risk Theory}
\begin{outline}
\1 The \textbf{marginal utility} of utility $u$ is defined by $u''$. Therefore, a
decreasing marginal utility implies a risk-averse utility.
\reff{Eeckhoudt, p.9}

\1 \textbf{Jensen's Inequality.} If $g$ is a convex function and $X$ is a
random variable, then
\begin{equation*}
  g(E[X]) \leq E[g(X)].
\end{equation*}
Conversely, if $g$ is concave, then
\begin{equation*}
  g(E[X]) \geq E[g(X)].
\end{equation*}

\1 The \textbf{Taylor expansion} of a function around the $E[X]$:
\begin{equation*}
  f(x) = f(E[X]) + \tfrac{1}{2}\Var(X)u''(\xi(x)).
\end{equation*}

\1 If $g$ is convex and $g(0)\leq0$, then $g$ is \textbf{supperadditive}, ie.
\begin{equation*}
  g(a) + g(b) \leq g(a+b).
\end{equation*}


\end{outline}

\part{Probability and Statistics}

\section{Basic Probability Rules}
\begin{outline}
Let $A$ and $B$ be two events, possibly overlapping. Then,
\1 \textbf{Disjunction (or).} 
\begin{equation*}
  P(A\vee B) = P(A) + P(B) - P(A \wedge B)
\end{equation*}
\1 \textbf{Conjunction (and).}
\begin{equation*}
  P(A\wedge B) = P(A)\times P(B|A)
\end{equation*}
\end{outline}

\section{Definitions}
\begin{outline}
\1 The \textbf{cumulative distribution function} or \textbf{CDF} of a random variable is
defined by
\begin{equation*}
  F_X(x) = P(X\leq x).
\end{equation*}
We also have the following identity:
\begin{equation*}
  P(a<X\leq b) = F_X(b) - F_X(a).
\end{equation*}

\1 If $X$ is a continuous random variable, then its \textbf{probability density function}
or \textbf{PDF} is defined by
\begin{equation*}
  f_X(x) = \frac{d}{dx}F_X(x).
\end{equation*}
Equivalently, we also have
\begin{equation*}
  F_X(x) = \int_{-\infty}^x f_X(t)\,dt.
\end{equation*}

\1 By definition of the PDF, 
\begin{equation*}
  P(a\leq X\leq B) = \int_{a}^bf_X(x)\,dx.
\end{equation*}

\1 The \textbf{quantile function} $Q:[0,1]\to R$, $R$ being the range of $X$, is given, in
the continuous and nice case by:
\begin{equation*}
  Q(p) = F_X^{-1}(p),
\end{equation*}
ie the value x such that 
\begin{equation*}
  F_X(x) = P(X\leq x) = p,
\end{equation*}
and in the general case by
\begin{equation*}
  Q(p) = \inf\{x: p\leq F_X(x)\}.
\end{equation*}

\1 The quantile of $q_p$ of $X\sim\normal(\mu,\sigma^2)$ is obtained by using
$(X-\mu)/\sigma$, ie. $\mu + \sigma q_p(\normal(0,1))$.

\end{outline}

\section{Distribution Theory}

\begin{outline}
\1 A \textbf{probability space} $(\Omega, \mathscr{F}, P)$ is a set of three parts:
\begin{enumerate}
\item A sample space $\Omega$ of all possible outcomes;
\item A set of events $\mathscr{F}$;
\item An assignment of probability $P$ for each of these events.
\end{enumerate}

\1 The \textbf{expected value} of a random variable $X$ with density $f$ is defined by
\begin{equation*}
  E[X] = \int x\,f(x)\,dx.
\end{equation*}
In general, if $X$ is defined on probability space $(\Omega,\mathscr{F},P)$, then 
\begin{equation*}
  E[X] = \int_\Omega X\,dP = \int_\Omega X(\omega)P(d\omega).
\end{equation*}

\1 The \textbf{law of total expectation} states that
\begin{equation*}
  E[X] = E[E[X|\mathscr{F}_t]].
\end{equation*}

\1 The \textbf{variance} of a random variable $X$ is defined by
\begin{align*}
  \Var(X) &= E[(X-E[X])^2]\\
  \Var(X) &= E[X^2] - E[X]^2.
\end{align*}

\1 The \textbf{covariance} of two random variables is defined by
\begin{align*}
  \Cov(X,Y) &= E[(X-E[X])(Y-E[Y])]\\
            &= E[XY] - E[X]E[Y].
\end{align*}

\1 The \textbf{correlation} of two random variables is defined by
\begin{equation*}
  \Corr(X,Y) = \frac{\Cov(X,Y)}{\Var(X)\Var(Y)}.
\end{equation*}

\1 The \textbf{covariance matrix} $\Sigma$ of random variables $X_i$ is defined by
\begin{gather*}
  \Sigma_{ij} = \Cov(X_i,X_j)\\
  \Sigma = E[(X-E[X])(X-E[X])^T]\\
  \Sigma = \diag(\sigma)R\diag(\sigma).
\end{gather*}

\1 The \textbf{correlation matrix} $R$ of random variables $X_i$ is defined by
\begin{gather*}
  R_{i}= \Corr(X_i,X_j)\\
  R = \diag(\Sigma)^{-1/2}\Sigma\diag(\Sigma)^{-1/2}.
\end{gather*}

\1 The \textbf{skewness} $\gamma_1$ of a random variable $X$ is defined by
\begin{equation*}
  \gamma_1 = \frac{1}{\sigma^3}E[(X-\mu)^3].
\end{equation*}

\1 The \textbf{kurtosis} $\gamma_2$ of a random variable $X$ is defined by
\begin{equation*}
  \gamma_2 = \frac{1}{\sigma^4}E[(X-\mu)^4].
\end{equation*}

\1 The \textbf{characteristic function} $\varphi_X(t)$ of a random variable $X$ is defined by
\begin{equation*}
  \varphi_X(t) = E[\exp(itX)].
\end{equation*}

\1 The \textbf{moment generation function} $M_X(t)$ of a random variable $X$ is defined by
\begin{equation*}
  M_X(t) = E[\exp(tX)].
\end{equation*}
Let $m_i$ be the $i$\textsuperscript{th} moment of $X$. Then,
\begin{equation*}
  M_X(t) = 1+tm_1 + \tfrac{1}{2!}t^2m_2 + \tfrac{1}{3!} t^3m_3 + \cdots
\end{equation*}

\1 The \textbf{cumulants} $\kappa_n$ of a random variable $X$ are defined by
\begin{equation*}
  \kappa_n(X) = \left.\frac{d^n}{dt^n}\log(M_X(t))\right\vert_{t=0}.
\end{equation*}
The following rule holds
\begin{equation*}
  \kappa_n(aX + bY) = a^n\kappa_n(X) + b^n\kappa(Y).
\end{equation*}
We also have
\begin{align*}
  \kappa_1(X) &= E[X]=\mu\\
  \kappa_2(X) &= \Var(X) = \sigma^2\\
  \kappa_3(X) &= E[(X-\mu)^3]\\
  \kappa_4(X) &= E[(X-\mu)^4] - 3\sigma^4.
\end{align*}
In particular, the skewness $\gamma_1 = \kappa_3/\sigma^3$ and the kurtosis $\gamma_2 =
3+\kappa_4/\sigma^4$. 
\end{outline}


\section{Stochastic Calculus}

\begin{outline}
  \1 A \textbf{brownian motion} has independant increments and $W_t \sim \normal(0,t)$
  (variance of $t$, so standard deviation $\sqrt{t}$).

  \1 A $d$-dimensional brownian processes $W$ is correlated with matrix $R$ if
  \begin{equation*}
    W_t - W_s \sim \normal_d(0, R(t-s)).
  \end{equation*}

  % \1 In particular, two brownian processes $V$ and $W$ have correlation $\rho$ if

  \1 The \textbf{reflection principle} states that 
  \begin{equation*}
    P(M_t \geq a) = 2P(W_t \geq a),
  \end{equation*}
  where 
  \begin{equation*}
    M_t = \sup_{0\leq s \leq t} W_t.
  \end{equation*}
\end{outline}

\section{Distributions}

\subsection{Normal Distribution}

\begin{center}
\begin{tabular}{ll} \toprule
  Notation & $\mathscr{N}(\mu,\sigma^2)$\\
  Mean & $\mu$\\
  Variance & $\sigma^2$\\
  Skewness & $0$\\
  Kurtosis & $3$\\
  PDF & $\displaystyle
        \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$\\
  CDF & $\displaystyle
        \frac{1}{2}\left(1+\erf\left(\frac{x-\mu}{\sigma\sqrt{2}}\right)\right)$\\
  Quantile & $\mu + \sigma\sqrt{2}\erf^{-1}(2q-1)$\\
  Quantile $\{0.01,0.99\}$ & $\pm 2.32635$\\
  Quantile $\{0.025,0.975\}$ & $\pm 1.95996$\\
  \bottomrule
\end{tabular}
\end{center}

\begin{outline}
  \1 Linear combination of normal variables remains normal:
  \begin{equation*}
    \sum_i a_i X_{\normal(\mu_i,\sigma^2_i)} \sim \normal(\sum_ia_i\mu_i,\sum_i(a_i \sigma_i)^2).
  \end{equation*}
\end{outline}

\subsection{Lognormal Distribution}
\begin{center}
\begin{tabular}{ll}
  \toprule
  Notation & $\logNormal(\mu,\sigma^2)$\\
  Mean & $\exp(\mu+\sigma^2/2)$\\
  Variance & $(\exp(\sigma^2)-1)\exp(2\mu+\sigma^2)$\\
  \bottomrule
\end{tabular}
\end{center}

\begin{outline}
  \1 By definition,
  \begin{equation*}
    \exp X_{\normal(\mu,\sigma^2)} \sim \logNormal(\mu,\sigma^2).
  \end{equation*}

  \1 The following properties hold:
  \begin{gather*}
    a X_{\logNormal(\mu,\sigma^2)} \sim \logNormal(\mu+\log a,\sigma^2)\\
    E[\exp(aX_{\normal(\mu,\sigma^2)})] = \exp(a\mu + \tfrac{1}{2}(a\sigma)^2).
  \end{gather*}

\end{outline}


\subsection{Chi-Squared Distribution}
\begin{outline}
  \1 $\chi^2(\nu)$ is the distribution of the sum of the squares of $\nu$ independant
  standard normal random variables, ie. $\sum_i Z^2_i$, $Z_i\sim\mathscr{N}(0,1)$.

  \1 
  \begin{tabular}{ll} \toprule
    Mean & $\nu$\\
    Variance & $2\nu$\\
    PDF & $\displaystyle \frac{1}{\Gamma(\nu/2)2^{\nu/2}}\exp(-x/2) \bm
          1_{x>0}$\\
    Cumulants & $\nu 2^{n-1}(n-1)!$\\
    \bottomrule
  \end{tabular}
\end{outline}

\subsection{Gamma Distribution}
\begin{outline}
  \1 The gamma distribution $\GammaDist(\alpha,\beta)$ (or $\GammaDist(k,\theta)$ on
  wikipedia EN) is the generalization of 
  \2 The exponential distribution:
  $\GammaDist(1,1/\lambda) = \Exp(\lambda)$; 
  \2 The chi-squared distribution:
  $\GammaDist(\nu/2,2) = \chi^2(\nu)$.

  \1 If $X_1,\dots,X_n$ are independent and $X_i\sim\GammaDist(\alpha_i,\beta)$, then
  \begin{equation*}
    \sum_iX_i \sim \GammaDist(\sum_i \alpha_i,\beta).
  \end{equation*}

  \1 
  \begin{tabular}{ll}
    \toprule
    Mean & $\alpha\beta$\\
    Variance & $\alpha\beta^2$\\
    Skewness & $2/\sqrt{\alpha}$\\
    Kurtosis & $6/\alpha + 3$\\
    PDF & $\displaystyle \frac{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1} \exp(-x/\beta)
          \bm 1_{x>0}$\\
    \bottomrule
  \end{tabular}
\end{outline}


\section{Estimation Theory}

\subsection{Maximum Likelihood Estimation}
\begin{outline}
  \1 The \textbf{maximum likelihood estimation} for a parameter $\theta \in \real^p$
  converges to a multivariate gaussian distribution:
  \begin{equation*}
    \sqrt{n}(\hat\theta_n - \theta) \leadsto \normal(0,\Sigma),
  \end{equation*}
  where $\Sigma$ is the \textbf{Penrose inverse} of $\hat I_\theta$, the \textbf{Fisher
    information matrix}. In pratice, we can approximate $\hat I_\theta = n^{-1}\hat H_n$
  the \textbf{hessian} at $\hat\theta_n$ so that
  \begin{equation*}
    \Sigma = n\hat H_n^{-1}.
  \end{equation*}

  \1 The \textbf{Slutzky theorem} states that if 
  \begin{equation*}
    \sqrt{n}(\hat\theta_n - \theta) \leadsto \normal_m(0,\Sigma),
  \end{equation*}
  and $f:\real^m\to\real^p$, then 
  \begin{equation*}
    \sqrt{n}(f(\hat\theta_n) - f(\theta)) \leadsto \normal_p(0,Df(\theta)\,\Sigma\,Df(\theta)^T),
  \end{equation*}
  where $Df(\theta)\in\real^{p\times m}$ is the \textbf{jacobian} of $f$. In pratice,
  $Df(\theta)$ is estimated by $Df(\hat\theta_n)$.
\end{outline}


\section{Copulas and Multivariate Dependence}
\begin{outline}

\1 The \textbf{margin} of a multivariate distribution is the distribution of a single
random variables when all others are known to be true. 

\1 In the bivariate case, if $(X_1,X_2)\sim F$ then $F$ is the \textbf{joint distribution}
and 
\begin{gather*}
F_1(x_1) = F(x_1,\infty),\\
F_2(x_2) = F(\infty,x_2)  
\end{gather*}
are the \textbf{marginal distributions}. They are such that $X_1\sim F_1$ and $X_2\sim
F_2$.
\reff{Remillard, Sec. 8.3.2 p. 269}

\1 The \textbf{rank} of an observation $(x_i,y_i)$ out of sample
$\{(x_1,y_1),\dots,(x_N,y_N)\}$ is the pair $(r_{x_i},r_{y_i})$ where $r_{x_i}$ is the
rank among $x_j$. The \textbf{normalized rank} is the pair $(r_{x_i},r_{y_i})/(N+1)$.
\reff{Remillard, Sec. 8.3.3 p. 269}

\1 \underline{Theorem.} If $F_1$ and $F_2$ are two distributions, $F_1$ being symmetric,
and the variance of $X_1\sim F_1$ and $X_2\sim F_2$ exist, then $X'_1=F_1^{-1}(U_1)\sim
F_1$, $X'_2=F_2^{-1}(U_2)\sim F_2$ and $\Corr(X'_1,X'_2)=0$.
\reff{Remillard, Prop. 8.4.1, p.272}

\1 The \textbf{Rosenblatt} transform of a copula $c(u,v)$ is given by
\begin{equation*}
  \psi(u,v) = \left(u,\frac{\partial_u c(u,v)}{\partial_u c(u,1)}\right).
\end{equation*}
Therefore, if $(U,V)\sim C$, then the following identity holds
\begin{equation*}
  \psi(U,V) \sim C_\perp.
\end{equation*}

\1 For any \textbf{elliptical copula}, the relation between the \textbf{correlation
  matrix} $\rho$ and the \textbf{Kendall tau} is given by
\begin{equation*}
  \tau = \frac{2}{\pi}\arcsin(\rho).
\end{equation*}
In particular, in the bivariate case, if $\tau=1/2$, then $\rho=2^{-1/2}$.
\reff{p.298}

\1 For \textbf{bivariate archimedean copulas}, the relation between the \textbf{generator}
$\phi(t)$ and the \textbf{Kendall tau} is given by
\begin{equation*}
  \tau = 1 + 4\int_0^1\frac{\phi(t)}{\phi'(t)}\,dt.
\end{equation*}

\1 The \textbf{archimedean generator} $\phi(t)$ is such that
\begin{equation*}
  c(u,v) = \phi^{-1}(\phi(u) + \phi(v)).
\end{equation*}
\end{outline}

\subsection{Copula Families}

\subsubsection{The Frank Copula [p.~306]}
\begin{outline}
  \1 The \textbf{Frank copula} for $d=2$ is given by
  \begin{equation*}
    C(u,v) = \frac{1}{\log\theta}\log\left(\frac{\theta + \theta^{u+v} - \theta^u - \theta^v}{\theta-1}\right).
  \end{equation*}

  \1 With $d=2$, the following identities hold:
  \begin{gather*}
    C_1 = C_\perp\\
    C_0 = C_{+}\\
    C_\infty = C_{-}
  \end{gather*}
\end{outline}

\subsubsection{The Clayton Copula [p.~304]}
\begin{outline}
  \1 In the bivariate case, the \textbf{Clayton copula} is given by
  \begin{gather*}
    C(u,v) = (u^{-\theta} + v^{-\theta} - 1)^{-1/\theta}, \qquad 0<\theta\\
    C(u,v) = (\max(0, u^{-\theta} + v{-\theta} - 1))^{-1/\theta},\qquad -1/2\leq\theta<0.
  \end{gather*}

  \1 For $d=2$, the \textbf{generator} is given by
  \begin{equation*}
    \phi(t) = \frac{t^{-\theta}-1}{\theta},
  \end{equation*}
  for $\theta>2$.

  \1 With $d=2$, the following identites hold:
  \begin{gather*}
    C_0 = C_\perp\\
    C_\infty = C_{+}\\
    C_{-1} = C_{-}.
  \end{gather*}

  \1 The relation between $\theta$ and $\tau$ for $d=2$ is given by
  \begin{gather*}
    \tau = \frac{\theta}{\theta+2}\\
    \theta = \frac{2\tau}{1-\tau}.
  \end{gather*}

  \1 This copula family is only \textbf{valid} for
  \begin{equation*}
    \theta\geq-\frac{1}{d-1}.
  \end{equation*}
\end{outline}

\subsubsection{The Gumbel Copula [p.~305]}
\begin{outline}
  \1 In the bivariate case, for $0<\theta\leq1$, the \textbf{Gumbel copula} is given by
  \begin{equation*}
    C(u,v) = \exp(-((-\log u)^{1/\theta} + (-\log v)^{1/\theta})^\theta)
  \end{equation*}

  \1 Its generator is given by
  \begin{equation*}
    \phi(t) = (-\log t)^{1/\theta}.
  \end{equation*}

  \1 The following identities hold:
  \begin{gather*}
    C_1 = C_\perp\\
    C_0 = C_{+}.
  \end{gather*}

  \1 The relation between $\theta$ and $\tau$ is given by
  \begin{equation*}
    \tau = 1-\theta.
  \end{equation*}
\end{outline}


\section{Calculus}
\begin{outline}
\1 The \textbf{chain rule} of $h(x) = g(f(x))$, where $f:\real^n\to\real$ and
$g:\real\to\real$ is given by
\begin{equation*}
\grad h(x) = g'(f(x))\grad f(x).  
\end{equation*}
\end{outline}


\section{Linear Algebra}
\begin{outline}
  \1 The inverse of a $2\times2$ matrix is given by
  \begin{equation*}
    \begin{pmatrix}a&b\\c&d\end{pmatrix}^{-1} = \frac{1}{ad-bc}\begin{pmatrix}d&-b\\-c&a\end{pmatrix}.
  \end{equation*}

  \1 The \textbf{Cholesky decomposition} of the semi-definite matrix $M$ is the
  \textbf{upper triangular} matrix $L$ such that
  \begin{gather*}
    M = L^TL\\
    L = \chol M.
  \end{gather*}
\end{outline}


%\newpage
\part{Finance Theory}

\section{Black-Scholes Model}
\begin{outline}
  \1 Under the \textbf{Black-Scholes Model}, the evolution of $d$ assets $S_t\in\real^d$ is
  given elementwise by
  \begin{equation*}
    S_t = S_0\exp((\mu - \tfrac{1}{2}\sigma^2)t + \sigma W_t),
  \end{equation*}
  with $\mu,\sigma\in\real^d$ and where $W$ is a d-brownian process with \textbf{correlation
  matrix} $R$, or, equivalently, covariance matrix
  \begin{equation*}
    \Sigma=\diag(\sigma)R\diag(\sigma).
  \end{equation*}

  \1 With $b=\chol R$ and $\tilde W$ an \textbf{uncorrelated} d-brownian process, we have
  \begin{equation*}
    S_t = S_0\exp((\mu - \tfrac{1}{2}\sigma^2)t + \diag(\sigma)b^T\tilde W_t).
  \end{equation*}

  \1 Equivalently, with $a=\chol\Sigma = b\diag\sigma$,
  \begin{equation*}
    S_t = S_0\exp((\mu - \tfrac{1}{2}\sigma^2)t + a^T\tilde W_t).
  \end{equation*}

  \1 Matlab code, with $n=h^{-1}=252$ looks like
  \begin{lstlisting}[language=Matlab,basicstyle=\footnotesize,numberstyle=\tiny]
    a = chol(Sigma);
    Z = randn(n,d);
    X = h*ones(n,1)*(mu - 0.5*vol.^2)' - sqrt(h)*Z*a;
    S = 100*exp(cumsum(X)); % nxd matrix
  \end{lstlisting}

  \1 In the bivariate case, the following relations hold:
  \begin{gather*}
    a = \begin{pmatrix}\sigma_1 & \rho\sigma_2\\0&\sqrt{1-\rho^2}\sigma^2\end{pmatrix}\\
    \sigma_1 = a_{11}\\
    \sigma_2 = \sqrt{a_{22}^2 + a_{12}^2}\\
    \rho = \frac{a_{12}}{\sqrt{a_{22}^2 + a_{12}^2}}\\
  \end{gather*}
\end{outline}

\subsection{Derivatives}
\begin{outline}
  \1 The value $c$ of a regular \textbf{european call} of an asset $S_t$ with strike $K$
  is given by
  \begin{gather*}
    c = S_0n(d_1) - Ke^{-rT}n(d_2)\\
    d_1 = \frac{\log(S_0/K) + rT + \tfrac{1}{2}\sigma^2T}{\sigma\sqrt{T}}\\
    d_2 = d_1 - \sigma\sqrt{T}.
  \end{gather*}

  \1 The \textbf{put-call parity} relates the value $c$ of an european call with the value
  $p$ of an \textbf{european put}:
  \begin{equation*}
    c - p = S_0 - Ke^{-rT}.
  \end{equation*}

  \1 The \textbf{exchange option} with parameters $q_1,q_2$ over two assets
  $S^{(1)}_t,S^{(2)}_t$ has payoff
  \begin{equation*}
    \Psi(S^{(1)}_t,S^{(2)}_t) = \max(q_2S^{(2)}_T - q_1S^{(1)}_T,0).
  \end{equation*}
  Its value $c$ is given by:
  \begin{gather*}
    c = q_2S^{(2)}_0\,n(d_1) - q_1S^{(1)}_0\,n(d_2)\\
    \sigma^2 = \sigma_1^2 + \sigma_2^2 - 2\rho\sigma_1\sigma_2\\
    d_1 = \frac{\log(q_2S^{(2)}_0 - \log(q_1S^{(1)}_0) +
      \tfrac{1}{2}T\sigma^2}{\sigma\sqrt{T}}\\
    d_1 = \frac{\log(q_2S^{(2)}_0 - \log(q_1S^{(1)}_0) -
      \tfrac{1}{2}T\sigma^2}{\sigma\sqrt{T}},
  \end{gather*}
  and its delta by
  \begin{gather*}
    \Delta_1 = q_2\,n(d1)\\
    \Delta_2 = -q_1\,n(d2).
  \end{gather*}
\end{outline}



\section{Interest Rates}

\subsection{Ornstein Uhlenbeck Process Model}
\begin{outline}
  \1 The \textbf{Ornstein-Uhlenbeck process} model of the \textbf{spot rate} $r_t$ is
  given by:
  \begin{equation*}
    dr_t = \alpha(\beta - r_t)dt + \sigma\,dW_t.
  \end{equation*}

  \1 The Ornstein-Uhlenbeck process is invariant to new measures
  $\tilde r(t) = \lambda r(mt)$. We have the following transformation rules:
  \begin{gather*}
    \tilde\alpha = m\alpha\\
    \tilde\beta = \lambda\beta\\
    \tilde\sigma = \sqrt{m}\lambda\sigma\\
    \tilde q_1 = \sqrt{m}q_1\\
    \tilde q_2 = \sqrt{m}\lambda^{-1}q_2
  \end{gather*}

  \1 In the \textbf{risk neutral measure}, if $r_t$ is also an Ornstein-Uhlenbeck process,
  and the \textbf{market price of risk} is given by
  \begin{equation*}
    q = q_1 + q_2r,
  \end{equation*}
  then
  \begin{equation*}
    d\tilde r_t = a(b - \tilde r_t)dt + \sigma\,d\tilde W_t.
  \end{equation*}

  \1 The transformation between the two measures is given by
  \begin{equation*}
    a = \alpha + q_2\sigma,\qquad b = \frac{\alpha\beta - q_1\sigma}{a}.
  \end{equation*}

  \1 Under the \textbf{number convention}, the 1\$ face value \textbf{zero-coupon bond
    valuation} under the risk neutral measure is given by
  \begin{equation*}
    P_T = \exp(A_T - r_0\,B_T).
  \end{equation*}
  Under the \textbf{percentage convention}, it is given by
  \begin{equation*}
    P_T = \exp(\tilde A_T - r_0\tilde B_T/100),
  \end{equation*}
  where
  \begin{gather*}
    B_T = \frac{1-\exp(-aT)}{a},\\
    \tilde B_T = \frac{1-\exp(-aT)}{a},\\
    A_T = \left(\frac{\sigma^2}{2a^2}-b\right)(T - B_T) - \frac{\sigma^2}{4a}B_T^2,\\
    \tilde A_T = \left(\frac{\sigma^2}{\num{20000}a^2}-\frac{b}{100}\right)(T - \tilde
    B_T) - \frac{\sigma^2}{\num{40000}a} \tilde B_T^2.
  \end{gather*}

  \1 Under the \textbf{percentage convention/yearly basis}, The \textbf{long term yield}
  is given by
  \begin{equation*}
    R_\infty = b - \frac{\sigma^2}{200a^2}.
  \end{equation*}

  \1 Under the \textbf{percentage convention/yearly basis}, the \textbf{relation between
    the spot rate and the yield} is given by
  \begin{gather*}
    r_0 = \frac{T\,R_T + 100 \tilde A_T}{\tilde B_T},\\
    R_T = \frac{r_0\tilde B_T - 100\tilde A_T}{T}.
  \end{gather*}
  
  \1 The Ornstein-Uhlenbeck is a \textbf{markovian, stationary and gaussian process}. Its
  law is given by
  \begin{equation*}
    r_t \sim \normal(\mu_t,\sigma^2_t)
  \end{equation*}
  with
  \begin{gather*}
    \mu_t = \beta + e^{-\alpha t}(r_0 - \beta)\\
    \sigma^2_t = \sigma^2\frac{1-e^{-2\alpha t}}{2\alpha}.
  \end{gather*}
  Furthermore, if $\alpha>0$, then the \textbf{stationnary distribution} of $r_t$ is
  \begin{equation*}
    r_\infty \sim \normal(\beta,\sigma^2/(2\alpha)).
  \end{equation*}
  Finally, 
  \begin{gather*}
    \Cov(r_t,r_s) = \sigma^2\frac{e^{-\alpha(t-s)}-e^{-\alpha(t+s)}}{2\alpha},\quad t>s\\
    \Cov(r_\infty,r_t) = 0.
  \end{gather*}
\end{outline}


\subsection{Cox-Ingersoll-Ross Model}
\begin{outline}
  \1 The \textbf{CIR model} of the spot rate $r_t$ is given by:
  \begin{equation*}
    dr_t = \alpha(\beta - r_t)dt + \sigma\sqrt{r_t}dW_t.
  \end{equation*}

  \1 The Feller process process is invariant to updated measures
  $\tilde r(t) = \lambda r(mt)$. We have the following transformation rules:
  \begin{gather*}
    \tilde\alpha = m\alpha\\
    \tilde\beta = \lambda\beta\\
    \tilde\sigma = \sqrt{m\lambda}\sigma\\
    \tilde q_1 = \sqrt{\lambda}q_1\\
    \tilde q_2 = \lambda^{-1/2}q_2\\
    \tilde q_2 = \lambda^{-1}q_2
  \end{gather*}
  The transformation with $m\neq1$ doesn't carry to $q$.

  \1 In the \textbf{risk neutral measure} with \textbf{market price of risk} given by
  \begin{equation*}
    q = q_1 r^{-1/2} + q_2 r^{1/2}
  \end{equation*}
  then 
  \begin{equation*}
    d\tilde r_t = a(b-\tilde r_t)dt + \sigma\sqrt{\tilde r_t}d\tilde W_t.
  \end{equation*}

  \1 The transformation between the two measures is given by
  \begin{equation*}
    a = \alpha + q_2\sigma,\qquad b=\frac{\alpha\beta - q_1\sigma}{a}.
  \end{equation*}

  \1 Under the \textbf{number convention}, the 1\$ face value \textbf{zero-coupon bond
    valuation} under the risk neutral measure is given by
  \begin{equation*}
    P_T = \exp(A_T - r_0B_T),
  \end{equation*}
  whereas under the \textbf{percentage convention}, it is given by
  \begin{equation*}
    P_T = \exp(\tilde A_T - r_0\tilde B_T/100),
  \end{equation*}
  where
  \begin{gather*}
    \gamma = \sqrt{a^2 + 2\sigma^2}\\
    \tilde\gamma = \sqrt{a^2 + 2\sigma^2/100}\\
    \eta = (\gamma+a)(1-e^{-\gamma T})+2\gamma e^{-\gamma T}\\
    \tilde \eta  = (\tilde\gamma + a)(1-e^{-\tilde\gamma T})+2\tilde\gamma e^{-\tilde\gamma T}\\
    B_T = \frac{2(1-e^{-\gamma T})}{\eta}\\
    \tilde B_T = \frac{2(1-e^{-\tilde\gamma T})}{\tilde\eta}\\
    A_T = \frac{2ab}{\sigma^2}\log\left(\frac{2\gamma
        e^{T(a-\gamma)/2}}{\eta}\right)\\
    \tilde A_T = \frac{2ab}{\sigma^2}\log\left(\frac{2\tilde\gamma
        e^{T(a-\tilde\gamma)/2}}{\tilde\eta}\right)
  \end{gather*}

  \1 Under the \textbf{percentage convention/yearly basis}, the \textbf{long term yield}
  is given by
  \begin{equation*}
    R_\infty = \frac{2ab}{a+\tilde\gamma}.
  \end{equation*}

  \1 Under the \textbf{percentage convention/yearly basis}, the \textbf{relation between
    the spot rate and the yield} is given by
  \begin{gather*}
    r_0 = \frac{T\,R_T + 100\tilde A_T}{\tilde B_T},\\
    R_T = \frac{r_0\tilde B_T - 100\tilde A_T}{T}.
  \end{gather*}
\end{outline}


\section{Lévy Models}

\subsection{Merton Model}
\begin{outline}
  \1 The \textbf{Merton model} has the following form:
  \begin{equation*}
    at + \sigma W_t + \sum_{i=1}^{N_t} \xi_j,
  \end{equation*}
  with $N_t$ a $\lambda$-Poisson process, $\xi_j \sim \normal(\gamma,\delta^2)$ and
  \begin{gather*}
    a = \mu -\lambda\kappa - \tfrac{1}{2}\sigma^2,\\
    \kappa=  \exp(\gamma+\delta^2/2) - 1.
  \end{gather*}

  \1 Under the \textbf{risk neutral measure}, the following transformation take place 
  \begin{gather*}
    K_\phi = e^{\zeta_0}E[e^{-\xi_i/2}]\\
    \tilde\lambda = K_\phi\lambda\\
    \tilde\sigma = \sigma\\
    \tilde\gamma = \gamma + \zeta_1\delta^2\\
    \tilde\delta = \delta\\
    \tilde\eta(x) = e^{\phi(x)}K_\phi^{-1}\eta(x),
  \end{gather*}
  with $\eta(x)$ the density of the jumps $\xi_1\sim\normal(\gamma,\delta^2)$, ie.
  \begin{equation*}
    \eta(x) = \frac{1}{\delta\sqrt{2\pi}}\exp\left(-\frac{(x-\gamma)^2}{2\delta^2}\right).
  \end{equation*}
\end{outline}

\subsection{Kou Model}

\begin{outline}
  \1 The \textbf{Kou Model} has the following paramaters:
  \begin{gather*}
    a = \mu - \lambda\kappa - \tfrac{1}{2}\sigma^2,\\
    \kappa = \frac{1+p\eta_2+(p-1)\eta_1}{(\eta_1-1)(\eta_2+1)},\\
    \eta(x) = p\eta_1 e^{-\eta_1 x} \bm 1_{x>0} + (1-p)\eta_2 e^{-\eta_2x}\bm 1_{x\leq 0}.
  \end{gather*}

  \1 Under the \textbf{weighted-symmetric representation}, we have
  \begin{gather*}
    \zeta = \delta Y_{\Exp(1)}\\
    \delta = \eta_2^{-1}\\
    \theta = 0\\
    \omega = \eta_2/\eta_1.
  \end{gather*}
\end{outline}


\subsection{Jump Diffusion Models}
\begin{outline}
  \1 For \textbf{jump diffusion models} of the form
  \begin{equation*}
    X_t = at + \sigma W_t + \sum_{j=1}^{N_t}\xi_j,
  \end{equation*}
  the following identities hold
  \begin{gather*}
    \nu = \lambda\eta\\
    \kappa = E[\exp(\xi_1)-1] = \int(e^x-1)\nu(dx)\\
    a = \mu -\lambda\kappa -\tfrac{1}{2}\sigma^2.
  \end{gather*}

  \1 The jump diffusion processes are always represented using their \textbf{natural
    characteristics}.

  \1 The $\kappa$ parameter is interpreted as the average jump size, $\lambda$ is the
  average frequency of the jumps.

  \1 The \textbf{martingale measure} parameters of jump diffusion processes obtained with
  $U_{b,\phi}$ (p.~200) are given by
  \begin{gather*}
    K_\phi = E[\exp(\phi(\xi_1))]\\
    \tilde\lambda = \lambda K_\phi\\
    \tilde\nu = \tilde\lambda\tilde\eta\\
    \tilde\eta = e^{\phi(x)}K_\phi^{-1}\eta(x)\\
    \tilde\sigma = \sigma\\
    \tilde\kappa = E[\exp(\tilde\xi_1)-1] = \int(e^x-1)\tilde\nu(dx)\\
    \tilde a = a + b\sigma +\int_{-1}^1x(e^{\phi(x)}-1)\nu(dx)\\
    \tilde a^{\text{(nat)}} = a^{\text{(nat)}} + b\sigma
  \end{gather*}

  \1 The following relation also holds:
  \begin{align*}
    r &= a + b\sigma + \tfrac{1}{2}\sigma^2 + \int(e^x-1)\tilde\nu(dx)\\
      &= \mu + b\sigma - \lambda\kappa + \tilde\lambda\tilde\kappa
  \end{align*}
\end{outline}
% \subsection{Lévy Characteristics}
% \begin{outline}
%   \1 The set $\{a,\sigma^2,\nu\}$ are the \textbf{natural characteristics} of a Lévy
%   process $X_t$ if its characteristic function $\psi(u)$ satisfies
%   \begin{equation*}
%     \psi(u) = iau - \tfrac{1}{2}\sigma^2u^2 + \int(e^{iux}-1)\nu(dx).
%   \end{equation*}
% \end{outline}

\section{Stochastic Volatility Models}

\subsection{Discrete Models}
\begin{outline}
  \1 The considered models have the form
  \begin{align*}
    X_t &= \mu_t + \sigma_te_t\\
        &= \mu_t + \sqrt{h_t}e_t
  \end{align*}
  where 
  \begin{gather*}
    E[e_t|\filtr_{t-1}]=0\\
    \Var[e_t|\filtr_{t-1}] = 1,
  \end{gather*}
  and we have the correspondance
  \begin{equation*}
    \sigma^2_t = h_t.
  \end{equation*}

  \1 The \textbf{standard GARCH$(1,1)$} has the form
  \begin{equation*}
    h_t = \omega + \alpha h_{t-1}e^2_{t-1} + \beta h_{t-1}.
  \end{equation*}
\end{outline}

\end{document}  



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
